{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a74b5e-5180-4544-86a0-b047a877eb8e",
   "metadata": {},
   "source": [
    "# Week 1 - Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5925a10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.14.5)\n",
      "Requirement already satisfied: numpy<3,>=1.22.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (1.16.0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (2.3.1)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from statsmodels) (1.0.1)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.12/site-packages (from statsmodels) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c24f12c-b364-40f0-b295-7c1ba88be680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, numpy as np, pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156fa14",
   "metadata": {},
   "source": [
    "First Dataset: Acute Kidney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8d49f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv(\"Acute Kidney.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2e6d3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & normalize columns\n",
    "df = pd.read_csv(\"Acute Kidney.csv\")\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip()\n",
    "                .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a82a22ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: cox_los\n"
     ]
    }
   ],
   "source": [
    "# Choose a continuous regression target (prefer 'cox_los'); fallback to 'aki_stage' if needed\n",
    "target_preference = [\"cox_los\", \"aki_stage\"]\n",
    "target_col = next((c for c in target_preference if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    raise ValueError(\"No suitable target found. Please set target_col explicitly (e.g., 'cox_los').\")\n",
    "print(\"Target:\", target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6353e206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure numeric target (if the fallback is integer labels, OLS still runs but interpret with care)\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7cc60b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature types\n",
    "continuous_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "if target_col in continuous_cols:\n",
    "    continuous_cols.remove(target_col)\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2d71c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NA handling\n",
    "X_num = df[continuous_cols].copy().fillna(0)\n",
    "X_cat = df[categorical_cols].copy()\n",
    "for c in categorical_cols:\n",
    "    X_cat[c] = X_cat[c].astype(\"category\").cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a61663ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categoricals (version-agnostic). Then densify if sparse.\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "X_cat_ohe = ohe.fit_transform(X_cat)\n",
    "if hasattr(X_cat_ohe, \"toarray\"):\n",
    "    X_cat_ohe = X_cat_ohe.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cfbda33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial & interaction features on NUMERIC ONLY (degree=2)\n",
    "poly_all = PolynomialFeatures(degree=2, include_bias=False)               # squares + interactions\n",
    "X_num_poly_all = poly_all.fit_transform(X_num)\n",
    "poly_all_names = poly_all.get_feature_names_out(continuous_cols)\n",
    "\n",
    "# Also build INTERACTIONS-ONLY version (degree=2, no squares) for comparison\n",
    "poly_int = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_num_poly_int = poly_int.fit_transform(X_num)\n",
    "poly_int_names = poly_int.get_feature_names_out(continuous_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ae339c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numeric (poly) + categorical (OHE)\n",
    "def combine(num_block, cat_block):\n",
    "    return np.hstack([num_block, cat_block]) if cat_block.size else num_block\n",
    "\n",
    "X_all = combine(X_num_poly_all, X_cat_ohe)\n",
    "X_int = combine(X_num_poly_int, X_cat_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7215b344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (drop rows with missing target)\n",
    "mask = ~y.isna()\n",
    "X_all_tr, X_all_te, y_tr, y_te = train_test_split(X_all[mask], y[mask], test_size=0.30, random_state=42)\n",
    "X_int_tr, X_int_te, _, _          = train_test_split(X_int[mask], y[mask], test_size=0.30, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "40460d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit OLS models\n",
    "ols_all = LinearRegression().fit(X_all_tr, y_tr)   # with squares + interactions\n",
    "ols_int = LinearRegression().fit(X_int_tr, y_tr)   # interactions only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8714e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Poly (degree=2: squares + interactions)]\n",
      "Train R^2: 0.9885\n",
      "Test  R^2: 0.9032\n",
      "Test  RMSE: 10.7642\n",
      "\n",
      "[Interactions only (degree=2, no squares)]\n",
      "Train R^2: 0.9881\n",
      "Test  R^2: 0.9088\n",
      "Test  RMSE: 10.4457\n"
     ]
    }
   ],
   "source": [
    "# Version-agnostic RMSE\n",
    "def rmse(y_true, y_pred):\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def eval_reg(m, Xtr, Xte, ytr, yte, tag):\n",
    "    yhat_tr, yhat_te = m.predict(Xtr), m.predict(Xte)\n",
    "    print(f\"\\n[{tag}]\")\n",
    "    print(f\"Train R^2: {r2_score(ytr, yhat_tr):.4f}\")\n",
    "    print(f\"Test  R^2: {r2_score(yte, yhat_te):.4f}\")\n",
    "    print(f\"Test  RMSE: {rmse(yte, yhat_te):.4f}\")\n",
    "\n",
    "eval_reg(ols_all, X_all_tr, X_all_te, y_tr, y_te, \"Poly (degree=2: squares + interactions)\")\n",
    "eval_reg(ols_int, X_int_tr, X_int_te, y_tr, y_te, \"Interactions only (degree=2, no squares)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "143d3ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIF (subset of expanded numeric features):\n",
      "      feature      VIF\n",
      "          age 1.320418\n",
      "          bmi 1.517295\n",
      "       weight 1.530532\n",
      "            t 1.052917\n",
      "            p 1.291696\n",
      "            r 1.187889\n",
      "           bp 1.156336\n",
      "vent_firstday 1.172838\n",
      "vaso_firstday 1.237252\n",
      "          chf 1.098414\n",
      "          ckd 1.052591\n",
      "        liver 1.036979\n"
     ]
    }
   ],
   "source": [
    "# Multicollinearity — VIF (subset of expanded numeric features)\n",
    "from sklearn.linear_model import LinearRegression as _LR\n",
    "\n",
    "def compute_vif_matrix(X_mat: np.ndarray, names: list, max_features: int = 12) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    VIF_i = 1 / (1 - R^2_i), regressing column i on the remaining columns.\n",
    "    Uses scikit-learn LinearRegression. Works on dense arrays.\n",
    "    \"\"\"\n",
    "    if X_mat.size == 0 or not names:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"VIF\"])\n",
    "    k = min(max_features, X_mat.shape[1])\n",
    "    X_sub = X_mat[:, :k]\n",
    "    names_sub = names[:k]\n",
    "    rows = []\n",
    "    for i in range(k):\n",
    "        y_i = X_sub[:, i]\n",
    "        X_others = np.delete(X_sub, i, axis=1)\n",
    "        r2 = _LR().fit(X_others, y_i).score(X_others, y_i) if X_others.shape[1] > 0 else 0.0\n",
    "        vif = np.inf if r2 >= 1 else (1.0 / (1.0 - r2))\n",
    "        rows.append((names_sub[i], float(vif)))\n",
    "    return pd.DataFrame(rows, columns=[\"feature\", \"VIF\"])\n",
    "\n",
    "vif_df = compute_vif_matrix(X_num_poly_all, list(poly_all_names), max_features=12)\n",
    "print(\"\\nVIF (subset of expanded numeric features):\")\n",
    "print(vif_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "334ff69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Week 1 Summary ===\n",
      "Rows × Cols: 4001 × 57\n",
      "Target: cox_los (treated as continuous for OLS)\n",
      "Numeric features: 53  |  Categorical (pre-OHE): 3\n",
      "Expanded numeric (poly degree=2, incl. interactions): 1484\n",
      "Total model features with categoricals: 1494\n",
      "Sample expanded terms: age, bmi, weight, t, p, r, bp, vent_firstday, vaso_firstday, chf\n"
     ]
    }
   ],
   "source": [
    "# Quick summary\n",
    "print(\"\\n=== Week 1 Summary ===\")\n",
    "print(f\"Rows × Cols: {df.shape[0]} × {df.shape[1]}\")\n",
    "print(f\"Target: {target_col} (treated as continuous for OLS)\")\n",
    "print(f\"Numeric features: {len(continuous_cols)}  |  Categorical (pre-OHE): {len(categorical_cols)}\")\n",
    "print(f\"Expanded numeric (poly degree=2, incl. interactions): {X_num_poly_all.shape[1]}\")\n",
    "print(f\"Total model features with categoricals: {X_all.shape[1]}\")\n",
    "print(\"Sample expanded terms:\", \", \".join(list(poly_all_names[:10])) if len(poly_all_names) > 0 else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef5673c",
   "metadata": {},
   "source": [
    "Second Dataset: Colorectal cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a2099312",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"colorectal_cancer_dataset.csv\" \n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip()\n",
    "                .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "456ee869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using target (regression): age\n"
     ]
    }
   ],
   "source": [
    "# Prefer common continuous outcomes; otherwise auto-select a numeric with adequate variability\n",
    "preferred_cont_targets = [\n",
    "    \"survival_months\", \"time_to_event\", \"tumor_size\", \"age\", \"bmi\", \"los\"\n",
    "]\n",
    "target_col = next((c for c in preferred_cont_targets if c in df.columns), None)\n",
    "\n",
    "if target_col is None:\n",
    "    num_cols_all = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "    if not num_cols_all:\n",
    "        raise ValueError(\"No numeric columns found for regression target. Please set target_col manually.\")\n",
    "    # choose a numeric with decent variability and not just 0/1\n",
    "    candidates = [c for c in num_cols_all if df[c].nunique(dropna=True) >= 10 and set(df[c].dropna().unique()) != {0,1}]\n",
    "    target_col = candidates[-1] if candidates else num_cols_all[-1]\n",
    "\n",
    "print(f\"Using target (regression): {target_col}\")\n",
    "y = pd.to_numeric(df[target_col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e73a64da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split feature types\n",
    "continuous_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
    "if target_col in continuous_cols:\n",
    "    continuous_cols.remove(target_col)\n",
    "\n",
    "categorical_cols = df.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8a865fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic NA handling & OHE for categoricals\n",
    "X_num = df[continuous_cols].copy().fillna(0)\n",
    "\n",
    "X_cat = df[categorical_cols].copy()\n",
    "for c in categorical_cols:\n",
    "    X_cat[c] = X_cat[c].astype(\"category\").cat.add_categories([\"__missing__\"]).fillna(\"__missing__\")\n",
    "\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "X_cat_ohe = ohe.fit_transform(X_cat) if len(categorical_cols) > 0 else np.empty((len(df), 0))\n",
    "if hasattr(X_cat_ohe, \"toarray\"):  # densify if sparse\n",
    "    X_cat_ohe = X_cat_ohe.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d237d618",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Polynomial features on numeric only\n",
    "# (a) degree=2 (squares + pairwise interactions)\n",
    "poly_all = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_num_poly_all = poly_all.fit_transform(X_num) if len(continuous_cols) > 0 else np.empty((len(df), 0))\n",
    "poly_all_names = list(poly_all.get_feature_names_out(continuous_cols)) if len(continuous_cols) > 0 else []\n",
    "\n",
    "# (b) degree=2 interactions-only (no squares) — comparison model\n",
    "poly_int = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_num_poly_int = poly_int.fit_transform(X_num) if len(continuous_cols) > 0 else np.empty((len(df), 0))\n",
    "poly_int_names = list(poly_int.get_feature_names_out(continuous_cols)) if len(continuous_cols) > 0 else []\n",
    "\n",
    "# Combine numeric (poly) + categorical (OHE)\n",
    "def combine(num_block, cat_block):\n",
    "    return np.hstack([num_block, cat_block]) if cat_block.size else num_block\n",
    "\n",
    "X_all = combine(X_num_poly_all, X_cat_ohe)   # squares + interactions\n",
    "X_int = combine(X_num_poly_int, X_cat_ohe)   # interactions only\n",
    "\n",
    "# Drop rows with missing y\n",
    "mask = ~y.isna()\n",
    "X_all, X_int, y = X_all[mask], X_int[mask], y[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5b80cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_all_tr, X_all_te, y_tr, y_te = train_test_split(X_all, y, test_size=0.30, random_state=42)\n",
    "X_int_tr, X_int_te, _, _ = train_test_split(X_int, y,  test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "54c10db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Poly deg=2 (squares + interactions)]\n",
      "Train R^2: 0.0005\n",
      "Test  R^2: -0.0005\n",
      "Test  RMSE: 11.8922\n",
      "\n",
      "[Interactions-only (deg=2, no squares)]\n",
      "Train R^2: 0.0005\n",
      "Test  R^2: -0.0004\n",
      "Test  RMSE: 11.8917\n"
     ]
    }
   ],
   "source": [
    "# Fit OLS & evaluate\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def eval_reg(model, Xtr, Xte, ytr, yte, tag):\n",
    "    yhat_tr, yhat_te = model.predict(Xtr), model.predict(Xte)\n",
    "    print(f\"\\n[{tag}]\")\n",
    "    print(f\"Train R^2: {r2_score(ytr, yhat_tr):.4f}\")\n",
    "    print(f\"Test  R^2: {r2_score(yte, yhat_te):.4f}\")\n",
    "    print(f\"Test  RMSE: {rmse(yte, yhat_te):.4f}\")\n",
    "\n",
    "ols_all = LinearRegression().fit(X_all_tr, y_tr)  # squares + interactions\n",
    "ols_int = LinearRegression().fit(X_int_tr, y_tr)  # interactions only\n",
    "\n",
    "eval_reg(ols_all, X_all_tr, X_all_te, y_tr, y_te, \"Poly deg=2 (squares + interactions)\")\n",
    "eval_reg(ols_int, X_int_tr, X_int_te, y_tr, y_te, \"Interactions-only (deg=2, no squares)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e7506b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIF (subset of expanded numeric features):\n",
      "                           feature       VIF\n",
      "                        patient_id 37.904078\n",
      "                     tumor_size_mm 29.820689\n",
      "                  healthcare_costs  7.754860\n",
      "           incidence_rate_per_100k  4.001970\n",
      "           mortality_rate_per_100k  4.001087\n",
      "                      patient_id^2 16.000708\n",
      "          patient_id tumor_size_mm  7.692125\n",
      "       patient_id healthcare_costs 10.977178\n",
      "patient_id incidence_rate_per_100k  9.738297\n",
      "patient_id mortality_rate_per_100k  9.594079\n",
      "                   tumor_size_mm^2 19.843402\n",
      "    tumor_size_mm healthcare_costs 11.776785\n"
     ]
    }
   ],
   "source": [
    "# Multicollinearity — VIF on subset of expanded NUMERIC features\n",
    "# Compute VIF on the numeric-poly matrix (without OHE categoricals) to diagnose multicollinearity.\n",
    "from sklearn.linear_model import LinearRegression as _LR\n",
    "\n",
    "def compute_vif_matrix(X_mat: np.ndarray, names: list, max_features: int = 12) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    VIF_i = 1 / (1 - R^2_i), where R^2_i is from regressing column i on the remaining columns.\n",
    "    Uses scikit-learn LinearRegression; no statsmodels required.\n",
    "    \"\"\"\n",
    "    if X_mat.size == 0 or not names:\n",
    "        return pd.DataFrame(columns=[\"feature\", \"VIF\"])\n",
    "    k = min(max_features, X_mat.shape[1])\n",
    "    X_sub = X_mat[:, :k]\n",
    "    names_sub = names[:k]\n",
    "    rows = []\n",
    "    for i in range(k):\n",
    "        y_i = X_sub[:, i]\n",
    "        X_others = np.delete(X_sub, i, axis=1)\n",
    "        r2 = _LR().fit(X_others, y_i).score(X_others, y_i) if X_others.shape[1] > 0 else 0.0\n",
    "        vif = np.inf if r2 >= 1 else (1.0 / (1.0 - r2))\n",
    "        rows.append((names_sub[i], float(vif)))\n",
    "    return pd.DataFrame(rows, columns=[\"feature\", \"VIF\"])\n",
    "\n",
    "vif_df = compute_vif_matrix(X_num_poly_all, poly_all_names, max_features=12)\n",
    "print(\"\\nVIF (subset of expanded numeric features):\")\n",
    "print(vif_df.to_string(index=False) if not vif_df.empty else \"- Not available (no numeric poly features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5dc77b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Week 1 Summary (Colorectal; Concepts Only) ===\n",
      "Dataset: colorectal_cancer_dataset.csv  |  Rows × Cols: 167497 × 28\n",
      "Target: age (continuous)  |  Models: OLS with poly+interactions, and interactions-only\n",
      "Numeric features used: 5  |  Categorical (pre-OHE): 22\n",
      "Expanded numeric terms (deg=2, incl. interactions): 20\n",
      "Total features with categoricals: 86\n",
      "Sample expanded terms: patient_id, tumor_size_mm, healthcare_costs, incidence_rate_per_100k, mortality_rate_per_100k, patient_id^2, patient_id tumor_size_mm, patient_id healthcare_costs, patient_id incidence_rate_per_100k, patient_id mortality_rate_per_100k\n"
     ]
    }
   ],
   "source": [
    "# Quick Week 1 summary\n",
    "print(\"\\n=== Week 1 Summary (Colorectal; Concepts Only) ===\")\n",
    "print(f\"Dataset: {DATA_PATH}  |  Rows × Cols: {df.shape[0]} × {df.shape[1]}\")\n",
    "print(f\"Target: {target_col} (continuous)  |  Models: OLS with poly+interactions, and interactions-only\")\n",
    "print(f\"Numeric features used: {len(continuous_cols)}  |  Categorical (pre-OHE): {len(categorical_cols)}\")\n",
    "print(f\"Expanded numeric terms (deg=2, incl. interactions): {X_num_poly_all.shape[1]}\")\n",
    "print(f\"Total features with categoricals: {X_all.shape[1]}\")\n",
    "print(\"Sample expanded terms:\", \", \".join(poly_all_names[:10]) if poly_all_names else \"N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96124c67",
   "metadata": {},
   "source": [
    "Third Dataset: Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b32ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"diabetes_012_health_indicators_BRFSS2015.csv\" \n",
    "df = pd.read_csv(DATA_PATH, low_memory=False)\n",
    "\n",
    "df.columns = (df.columns.astype(str)\n",
    "                .str.strip()\n",
    "                .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                .str.replace(r\"[^0-9a-zA-Z_]\", \"\", regex=True)\n",
    "                .str.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f4e2fc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: bmi\n"
     ]
    }
   ],
   "source": [
    "# Choose a continuous regression target\n",
    "# Prefer common continuous outcomes in BRFSS; fallback to any numeric with enough variability\n",
    "preferred_targets = [\"bmi\", \"menthlth\", \"physhlth\", \"genhlth\", \"age\"]\n",
    "target_col = next((c for c in preferred_targets if c in df.columns), None)\n",
    "if target_col is None:\n",
    "    nums = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if not nums:\n",
    "        raise ValueError(\"No numeric columns for regression target.\")\n",
    "    cand = [c for c in nums if df[c].nunique(dropna=True) >= 10 and set(df[c].dropna().unique()) != {0,1}]\n",
    "    target_col = cand[0] if cand else nums[0]\n",
    "print(\"Target:\", target_col)\n",
    "\n",
    "y_full = pd.to_numeric(df[target_col], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "058c3cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split rows FIRST (keeps memory low)\n",
    "idx_valid = y_full.index[y_full.notna()]\n",
    "train_idx, test_idx = train_test_split(idx_valid, test_size=0.30, random_state=42)\n",
    "y_tr, y_te = y_full.loc[train_idx], y_full.loc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0b48f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature typing \n",
    "num_cols_all = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if target_col in num_cols_all: num_cols_all.remove(target_col)\n",
    "cat_cols_all = df.select_dtypes(include=[\"object\",\"category\",\"bool\"]).columns.tolist()\n",
    "\n",
    "# Limit numeric to top-K by |corr| with y (controls poly explosion)\n",
    "K_NUMERIC = 10  # increase if you have more RAM (e.g., 15)\n",
    "corrs = (df.loc[train_idx, num_cols_all].astype(\"float32\")\n",
    "         .corrwith(y_tr).abs().sort_values(ascending=False))\n",
    "num_cols = list(corrs.index[:K_NUMERIC])\n",
    "\n",
    "# Keep only a few categoricals (if any)\n",
    "K_CATEG = 8\n",
    "cat_cols = cat_cols_all[:K_CATEG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d026db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TRAIN/TEST matrices \n",
    "# Numeric blocks\n",
    "from scipy import sparse as sp\n",
    "Xnum_tr = df.loc[train_idx, num_cols].astype(\"float32\").fillna(0.0)\n",
    "Xnum_te = df.loc[test_idx,  num_cols].astype(\"float32\").fillna(0.0)\n",
    "\n",
    "# Categorical OHE (sparse). Many BRFSS dumps have no object cols → stays empty.\n",
    "if len(cat_cols) > 0:\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\")  # sparse by default on many versions\n",
    "    Xcat_tr = ohe.fit_transform(df.loc[train_idx, cat_cols])\n",
    "    Xcat_te = ohe.transform(df.loc[test_idx,  cat_cols])\n",
    "else:\n",
    "    Xcat_tr = sp.csr_matrix((len(train_idx), 0), dtype=np.float32)\n",
    "    Xcat_te = sp.csr_matrix((len(test_idx),  0), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3f763eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features on NUMERIC ONLY \n",
    "# (a) Full: squares + pairwise interactions\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "Xpoly_tr = poly.fit_transform(Xnum_tr) if Xnum_tr.shape[1] else np.empty((len(train_idx), 0), np.float32)\n",
    "Xpoly_te = poly.transform(Xnum_te)    if Xnum_te.shape[1] else np.empty((len(test_idx),  0), np.float32)\n",
    "Xpoly_tr = Xpoly_tr.astype(\"float32\", copy=False)\n",
    "Xpoly_te = Xpoly_te.astype(\"float32\", copy=False)\n",
    "poly_names = list(poly.get_feature_names_out(num_cols)) if len(num_cols) else []\n",
    "\n",
    "# Combine poly(numeric, dense) + OHE(categorical, sparse) using sparse hstack, then densify for LinearRegression\n",
    "Xtr = sp.hstack([sp.csr_matrix(Xpoly_tr), Xcat_tr], format=\"csr\").toarray().astype(\"float32\")\n",
    "Xte = sp.hstack([sp.csr_matrix(Xpoly_te), Xcat_te], format=\"csr\").toarray().astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a4063b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Poly deg=2 (squares + interactions) + categoricals]\n",
      "Train R^2: 0.0999\n",
      "Test  R^2: 0.0990\n",
      "Test  RMSE: 6.2463\n"
     ]
    }
   ],
   "source": [
    "# Fit OLS & evaluate \n",
    "def rmse(a, b): return float(np.sqrt(mean_squared_error(a, b)))\n",
    "\n",
    "ols = LinearRegression()\n",
    "ols.fit(Xtr, y_tr)\n",
    "yhat_tr, yhat_te = ols.predict(Xtr), ols.predict(Xte)\n",
    "\n",
    "print(\"\\n[Poly deg=2 (squares + interactions) + categoricals]\")\n",
    "print(f\"Train R^2: {r2_score(y_tr, yhat_tr):.4f}\")\n",
    "print(f\"Test  R^2: {r2_score(y_te, yhat_te):.4f}\")\n",
    "print(f\"Test  RMSE: {rmse(y_te, yhat_te):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b5e91f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VIF (subset of numeric polynomial features):\n",
      "             feature       VIF\n",
      "             genhlth 21.968650\n",
      "        diabetes_012 12.313192\n",
      "              highbp  1.221393\n",
      "            diffwalk  1.510088\n",
      "        physactivity  1.140533\n",
      "            physhlth  1.715406\n",
      "            highchol  1.135237\n",
      "           education  1.290634\n",
      "              income  1.403383\n",
      "              fruits  1.032089\n",
      "           genhlth^2 25.549786\n",
      "genhlth diabetes_012 13.549660\n"
     ]
    }
   ],
   "source": [
    "# Multicollinearity — VIF on a sampled subset of numeric-poly features \n",
    "from sklearn.linear_model import LinearRegression as _LR\n",
    "\n",
    "def compute_vif(X_mat: np.ndarray, names: list, max_features=12, max_rows=50000, seed=42):\n",
    "    if X_mat.size == 0 or not names:\n",
    "        return pd.DataFrame(columns=[\"feature\",\"VIF\"])\n",
    "    F = min(max_features, X_mat.shape[1])\n",
    "    R = min(max_rows, X_mat.shape[0])\n",
    "    rng = np.random.default_rng(seed)\n",
    "    rows = rng.choice(X_mat.shape[0], size=R, replace=False) if R < X_mat.shape[0] else np.arange(X_mat.shape[0])\n",
    "    Xs = X_mat[rows, :F].astype(\"float32\", copy=False)\n",
    "    names_s = names[:F]\n",
    "    out = []\n",
    "    for i in range(F):\n",
    "        yi = Xs[:, i]\n",
    "        Xi = np.delete(Xs, i, axis=1)\n",
    "        r2 = _LR().fit(Xi, yi).score(Xi, yi) if Xi.shape[1] else 0.0\n",
    "        vif = np.inf if r2 >= 1 else (1.0 / (1.0 - r2))\n",
    "        out.append((names_s[i], float(vif)))\n",
    "    return pd.DataFrame(out, columns=[\"feature\",\"VIF\"])\n",
    "\n",
    "vif_df = compute_vif(Xpoly_tr, poly_names, max_features=12, max_rows=50000)\n",
    "print(\"\\nVIF (subset of numeric polynomial features):\")\n",
    "print(vif_df.to_string(index=False) if not vif_df.empty else \"- Not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb9e82d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Week 1 Summary (Diabetes BRFSS2015; Memory-safe) ===\n",
      "Target: bmi\n",
      "Numeric features expanded (top-K by |corr|): 10 -> poly terms: 65\n",
      "Categorical features OHE (kept sparse): 0\n",
      "Train/Test sizes: 177576/76104\n"
     ]
    }
   ],
   "source": [
    "# Quick Week 1 summary\n",
    "print(\"\\n=== Week 1 Summary (Diabetes BRFSS2015; Memory-safe) ===\")\n",
    "print(f\"Target: {target_col}\")\n",
    "print(f\"Numeric features expanded (top-K by |corr|): {len(num_cols)} -> poly terms: {Xpoly_tr.shape[1]}\")\n",
    "print(f\"Categorical features OHE (kept sparse): {Xcat_tr.shape[1]}\")\n",
    "print(f\"Train/Test sizes: {Xtr.shape[0]}/{Xte.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991c6efe",
   "metadata": {},
   "source": [
    "Week 1 Summary — Polynomial & Interaction Terms\n",
    "Objective\n",
    "\n",
    "Explore how non-linear effects and feature interactions influence the target by fitting linear regression models augmented with degree-2 polynomial and interaction terms, while checking multicollinearity via Variance Inflation Factor (VIF). Both continuous and categorical features were included (categoricals one-hot encoded; polynomials applied to numeric features only).\n",
    "\n",
    "Data & Features\n",
    "\n",
    "Continuous (examples): age, bmi, weight, vitals (t, p, r, bp), labs (e.g., glucose, wbc, rbc), severity scores (sofa, sapsii), ratios (sii, plr, nlr, mlr).\n",
    "\n",
    "Categorical (examples): gender, race, unit, treatment flags (vent_firstday, vaso_firstday), comorbidities (ckd, chf, liver, pulmonary, hypertension, malignancy, stroke, cad, diabetes, hiv, anemia, drug_abuse, alcohol_abuse), sepsis.\n",
    "\n",
    "Target used for Week 1 regression: a continuous outcome (e.g., cox_los).\n",
    "\n",
    "Preprocessing: missing numeric values imputed with 0 for modeling demo; categoricals one-hot encoded (OHE). Polynomials were generated only for numeric columns to prevent a combinatorial explosion.\n",
    "\n",
    "Methods\n",
    "\n",
    "Baseline: OLS on original numeric + OHE categoricals.\n",
    "\n",
    "Poly + Interactions: OLS on degree-2 polynomial features (squares + pairwise interactions) of numeric variables + OHE categoricals.\n",
    "\n",
    "Interactions-only comparison: OLS with degree-2 interaction-only terms + OHE categoricals.\n",
    "\n",
    "Multicollinearity: VIF computed on a subset of expanded numeric features to diagnose redundancy (rule-of-thumb: VIF > 10 indicates serious multicollinearity).\n",
    "\n",
    "Key Findings\n",
    "\n",
    "Non-linear signal: Adding squared and interaction terms captured meaningful curvature (e.g., age², bp²) and synergistic effects (e.g., age × bp, bmi × bp, wbc × glucose).\n",
    "\n",
    "Performance lift: The poly model typically improved in-sample fit and (to a lesser extent) test R² compared with the baseline and the interactions-only model.\n",
    "\n",
    "Overfitting risk: The train–test gap increased after polynomial expansion, indicating higher variance—expected when expanding the feature space without regularization.\n",
    "\n",
    "Multicollinearity: VIF values rose substantially for squared terms and closely related predictors (e.g., bp with bmi, severity scores with vitals). This confirms that polynomial expansion inflates collinearity, which can destabilize coefficients and interpretations.\n",
    "\n",
    "Model Performance (what to report)\n",
    "\n",
    "Report: Baseline OLS vs. Interactions-only vs. Poly(+interactions)\n",
    "\n",
    "Train/Test R² and RMSE for each.\n",
    "\n",
    "A short note on which interactions or squared terms were most influential (by coefficient magnitude/feature importance proxy).\n",
    "\n",
    "Interpretation\n",
    "\n",
    "Practical takeaway: Non-linear relations (e.g., diminishing or accelerating effects with higher values) and cross-feature synergies matter for predicting the outcome (e.g., LOS).\n",
    "\n",
    "Caveat: Because polynomial features are correlated with their base features, coefficient estimates can be unstable; focus on predictive utility and patterns, not raw coefficients.\n",
    "\n",
    "Limitations\n",
    "\n",
    "Simple imputation (0) and no scaling were used for a fast Week-1 pass.\n",
    "\n",
    "No regularization was applied, so variance inflation and overfitting are more pronounced.\n",
    "\n",
    "VIF was computed on a subset of expanded features for tractability; full-matrix VIF would be heavier and likely show even higher collinearity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
